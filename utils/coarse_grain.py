# -*- coding: utf-8 -*-
"""
Created on Thu Apr 29 15:26:42 2021

@author: Leyang Xue

"""
root_path = 'F:/work/work5_reductionability'

import sys 
sys.path.append(root_path+'/NetworkReduction')
import os
from utils import kplexes as kplex 
import networkx as nx 
import numpy as np 
import pandas as pd 
from collections import defaultdict
import random 
import scipy.stats 
import math
import pickle as pk
from multiprocessing import Pool
import scipy.sparse as sparse
import powerlaw 
import sympy

def load_network(dataset):
    '''
    load the netwrok with a edgelist format  

    Parameters
    ----------
    dataset : list
        original network data which is represent by the edgelist.

    Returns
    -------
    G : nx.Graph()
        return to a undirected simple conective graph that don't contain selfloop and multiply edges.

    '''
        
    G = nx.from_edgelist(dataset)
    largest_cc = max(nx.connected_components(G), key=len)
    largest_cc_G = G.subgraph(largest_cc)
    G=nx.Graph(largest_cc_G)
    G.remove_edges_from(nx.selfloop_edges(G))
    
    return G

def weightNetwork(edgelist):
    '''
    load the weighted netwrok with a edgelist format  

    Parameters
    ----------
    edgelist : list
        edgelist of weight network. In each row, it correspond to an edge e.g. [u v weight]. 

    Returns
    -------
    G : nx.Graph()
        Graph generated by networkx.

    '''
    G = nx.Graph()
    for edge in edgelist:
        Edge = (edge[0],edge[1],edge[2])
        G.add_weighted_edges_from([Edge])
   
    return G


def load(filename):
   '''
    load the dataset
    
    Parameters
    ----------
    filename : str 
       path that need to load the file, and the file is saved in dump function.

    Returns
    -------
    TYPE file type, is the same as the previous type 
       file.

    ''' 
   with open(filename, 'rb') as input_file:  
        try:
            return pk.load(input_file)
        
        except EOFError:
            return None
                    
                    
def mkdir(path):
    '''
    make a directory
    
    Parameters
    ----------
    path : str
        file path.

    Returns
    -------
    create the path if don't have path .

    '''
    folder = os.path.exists(path)
    if not folder:
        os.makedirs(path)
        print('--create new folder--:'+str(path))
    else:
        print('There is this folder!')
        

def NetworkInx(SizeCliques,path):
    '''
    Return the index of k-clique CGNs 
    
    Parameters
    ----------
    SizeCliques : array
        size of clique in reduction process from original network to 2-clique network.
    path : str
        path to load the network edgelist .

    Returns
    -------
    network_inx : dict  
        the index in filename of network edgelist for n-clique networks, 
        where n-clique networks denotes that cliques whose size is larger than n have been merged into a new nodes so as to not have (n+1)-order cliques
    Nweight : dict
        the number of edge whose weight is not one for n-clique networks.
    
    '''
    network_inx ={} #record the network index with for a k-clique CGNs
    Nweight = {} #count the number of weighted edges 
    for size in sorted(set(SizeCliques)):
        network_inx[size]=min(np.where(SizeCliques == size)[0])
        filename = path+'/'+str(network_inx[size])+'_reducedEdgelist'
        edgelist = np.array(load(filename))
        Nweight[size]= len(edgelist[edgelist[:,2]!=1])
        
    return network_inx, Nweight


def CriticalInfectRate(G):
    '''
    calculate the critical infection rate in terms of network structure
    
    Parameters
    ----------
    G : Graph
        network.

    Returns
    -------
    pc : float
        critical infection rate obtained by bond pecolation analysis.

    '''
    sequence = np.array([d for n, d in G.degree()])
    k = np.average(sequence)
    k_2 = np.average(np.power(sequence,2))
    pc = k/(k_2-k)
    
    return pc

def IdentifyCriInfectRate(SimuData):
    '''
    identify the critical infection rate in terms of numerical simulation
    
    Parameters
    ----------
    SimuDate : array
       simulation data of SIR spreading .

    Returns
    -------
    pc : float
        critical infection rate.
    '''
    Savg  = np.average(SimuData,axis=1)
    Sstd = np.std(SimuData,axis=1)
    cv = Sstd/Savg
    pc = np.argmax(cv)/100
    
    return cv,pc

def InfectCriticalCondition(theoritical_result):
    '''
    calculate the least infection probability (\hat{beta_k}) required for occupying all nodes in the k-clique by one infected node

    Parameters
    ----------
    theoritical_result : array
        analytical results

    Returns
    -------
    sizeClique : dict
        \hat{beta_k}.

    '''
    sizeClique ={}
    beta = np.arange(0,1.01,0.01)
    for i in np.arange(theoritical_result.shape[1]):
        cr = beta[min(np.where(theoritical_result[:,i]>0.995)[0])]
        sizeClique[i+2] = round(cr,2)
    
    return sizeClique

def MaxLikelihood(seq):
    '''
    estimate the power exponent using maximum likelyhood method 

    Parameters
    ----------
    seq : list
        sequence.

    Returns
    -------
    xmin : int
        minimum value.
    alpha : float
        power exponent.

    '''
    fit = powerlaw.Fit(seq, discrete = True)
    alpha = fit.power_law.alpha
    xmin = fit.power_law.xmin
    
    return xmin, alpha

def PowerExponent(G):
    '''
    calculate the degree exponent using the MLM

    Parameters
    ----------
    G : graph
        network.

    Returns
    -------
    gamma : float
        degree exponent.

    '''
    seq = [each[1] for each in nx.degree(G)]
    [xmin,gamma] = MaxLikelihood(seq)
    
    return gamma
    
def netStatistics(path,save_path):
    '''
    
    Calculate the basic structure properities of networks 

    Parameters
    ----------
    path : str
        path to load the network.
    save_path : str
        path to save the result.

    Returns
    -------
    data_index : dataframe
        basic structure properities of network .

    '''
    file_name = os.listdir(path)
    data_index = pd.DataFrame(index=file_name,columns=['number of node', 'number of edge', 'average degree','average of clustering','assortative','gamma','alpha'])

    #create the network
    for each in file_name:
       
        print('process datasets:',each)
       
        data = np.loadtxt(path+'/' + each)
        G = load_network(data)
        CliqueSize = [len(clique) for clique in nx.find_cliques(G)]#clique size sequence  
        [xmin,alpha] = MaxLikelihood(CliqueSize)
        
        data_index.loc[each,'number of node'] = len(G.nodes())
        data_index.loc[each,'number of edge'] = len(G.edges())
        data_index.loc[each,'average degree'] = 2*data_index.loc[each,'number of edge']/data_index.loc[each,'number of node']
        data_index.loc[each,'average of clustering'] = nx.average_clustering(G)
        data_index.loc[each,'assortative'] = nx.degree_assortativity_coefficient(G)
        data_index.loc[each,'gamma'] = PowerExponent(G)
        data_index.loc[each,'alpha'] = alpha
        
    data_index.to_csv(save_path+'/network_statistic.csv')
    
    return data_index
       
def CliqueDistribution(G):
    '''
    calculate the distribution of clique in the network 

    Parameters
    ----------
    G : graph
        network.

    Returns
    -------
    CliqueDistribution: dict

    '''
    
    CliqueSize = [len(clique) for clique in nx.find_cliques(G)] #clique size sequence  
    
    cliqueDistribution = {} 
    for cs in CliqueSize:
        if cliqueDistribution.get(cs)==None:
           cliqueDistribution[cs]=1/len(CliqueSize)
        else:
           cliqueDistribution[cs]+=1/len(CliqueSize) 
    
    return cliqueDistribution,CliqueSize


def _simple_transmission_(p):
    '''
    for a given p, return whether to infect or not

    Parameters
    ----------
    p : float
        infection rate.

    Returns
    -------
    bool
        wether to infect or not.

    '''
    if random.random() < p:
        return True
    else:
        return False
    
def SIR(G, alpha, mu, infecteds, tmin = 0, tmax = float('Inf')):
    """
    numerical simulation of SIR model
    
    Parameters
    ----------
    G : nx.graph()
        undirected connective network.
    alpha : float 
        infection rate, alpha in [0,1]
    mu : float 
        recover rate, mu in [0,1]
    infecteds : int
        initial infected seed node.
    tmin : int, optional
        initial time. The default is 0.
    tmax : TYPE, optional
        maximum excuation time. The default is float('Inf').

    Returns
    -------
    S : list
        the number of susceptible node from begin to end.
    I : list
        the number of infected node from begin to end.
    R : list
        the number of recovered node from begin to end.

    """
    N = G.order()
    if type(infecteds) == list:
       infecteds = set(infecteds)
    elif G.has_node(infecteds):
       infecteds = set([infecteds])
    
     
    t = [0]
    S = [N-len(infecteds)]
    I = [len(infecteds)]
    R = [0]
    Allinfecteds = []
    
    node_state = defaultdict(lambda : ([tmin], ['S'])) #record the state of nodes 
    susceptible = defaultdict(lambda : True)  #record the susceptible nodes  
    
    for u in infecteds:
        node_state[u][0].append(t[-1])
        node_state[u][1].append('I')
        susceptible[u] = False
        
    while infecteds and t[-1] < tmax-1:#infecteds and 
        
        #update the time
        t.append(t[-1]+1) 
        new_infected = set()
        new_recovered = set()
        
        for u in infecteds:
            for v in G.neighbors(u):
                if susceptible[v]:
                   if node_state[u][1][-1] == 'I' and _simple_transmission_(alpha):
                       node_state[v][1].append('I')
                       node_state[v][0].append(t[-1])
                       susceptible[v] = False
                       new_infected.add(v)
                       
            if  _simple_transmission_(mu):
                node_state[u][1].append('R') 
                node_state[u][0].append(t[-1])
                new_recovered.add(u)
                
        S.append(S[-1] - len(new_infected))
        I.append(len(infecteds)-len(new_recovered)+len(new_infected))
        R.append(R[-1] + len(new_recovered))
        Allinfecteds.extend(list(new_recovered))
        
        for re in new_recovered:
            infecteds.remove(re)
        for ad in new_infected:
            infecteds.add(ad)
    
    return S,I,R,Allinfecteds  
  
def SIR_TS_Nodes(G, alpha,mu,infecteds, tmin = 0, tmax = float('Inf')):
    """   
    numerical simulation of SIR model 
    
    However, it return the nodes staying at different state at each time
    
    Parameters
    ----------
    G : graph
        network.
    alpha : float
        infection rate.
    mu : float
        recover rate.
    infecteds : int
        initial seed .
    tmin : int, optional
        start time. The default is 0.
    tmax : TYPE, optional
        maximum infection times. The default is float('Inf').

    Returns
    -------
    S : list
        susceptible nodes over t.
    I : list
        infected nodes over t.
    R : list
        recovered node over t.
    """
    if G.has_node(infecteds):
        infecteds = set([infecteds])

    t = [0]
    S = [list(set(G.nodes())-set(infecteds))]
    I = [list(infecteds)]
    R = []
    
    node_state = defaultdict(lambda : ([tmin], ['S'])) #record the state of nodes 
    susceptible = defaultdict(lambda : True)  #record the susceptible nodes  
    
    for u in infecteds:
        node_state[u][0].append(t[-1])
        node_state[u][1].append('I')
        susceptible[u] = False
    
    while infecteds and t[-1] < tmax-1:
        
        #update the time
        t.append(t[-1]+1) 
        new_infected = set()
        new_recovered = set()
        
        for u in infecteds:
            for v in G.neighbors(u):
                if susceptible[v]:
                   if node_state[u][1][-1] == 'I' and _simple_transmission_(alpha):
                       node_state[v][1].append('I')
                       node_state[v][0].append(t[-1])
                       susceptible[v] = False
                       new_infected.add(v)
                       
            if  _simple_transmission_(mu):
                node_state[u][1].append('R') 
                node_state[u][0].append(t[-1])
                new_recovered.add(u)  
                
        S.append(list(set(S[-1])-set(new_infected)))
        I.append(list((set(infecteds)-set(new_recovered)) | set(new_infected)))
        R.append(list(set(new_recovered)))
        
        for re in new_recovered:
            infecteds.remove(re)
        for ad in new_infected:
            infecteds.add(ad)
        
    return S,I,R

def SIR_WG(G, alpha, mu, infecteds, tmin = 0, tmax = float('Inf')):
    '''
    numerical simulation of SIR model on a weighted network 

    Parameters
    ----------
    G : graph
        network.
    alpha : float
        infection rate.
    mu : float
        recovery rate.
    infecteds : list
        initial seeds.
    tmin : int, optional
        time to start the process. The default is 0.
    tmax : int, optional
        time to end the process. The default is float('Inf').

    Returns
    -------
    S : list
        the number of nodes staying S state over time.
    I : TYPE
        the number of nodes staying I state over time.
    R : TYPE
        the number of nodes staying R state over time.
    Allinfecteds : list
        all nodes that has been infected from the start of dynamical process.

    '''
    
    N = G.order()
    if G.has_node(infecteds):
        infecteds = set([infecteds])
            
    t = [0]
    S = [N-len(infecteds)]
    I = [len(infecteds)]
    R = [0]
    Allinfecteds = []
    
    node_state = defaultdict(lambda : ([tmin], ['S'])) #record the state of nodes 
    susceptible = defaultdict(lambda : True)  #record the susceptible nodes  
    
    for u in infecteds:
        node_state[u][0].append(t[-1])
        node_state[u][1].append('I')
        susceptible[u] = False
        
    while infecteds and t[-1] < tmax-1:#infecteds and 
        
        #update the time
        t.append(t[-1]+1) 
        new_infected = set()
        new_recovered = set()
        
        for u in infecteds:
            for v in G.neighbors(u):
                if susceptible[v]:
                   weight = G.get_edge_data(u,v)['weight']
                   p = 1-np.power((1-alpha),weight)
                   if node_state[u][1][-1] == 'I' and _simple_transmission_(p):
                       node_state[v][1].append('I')
                       node_state[v][0].append(t[-1])
                       susceptible[v] = False
                       new_infected.add(v)
                       
            if  _simple_transmission_(mu):
                node_state[u][1].append('R') 
                node_state[u][0].append(t[-1])
                new_recovered.add(u)
                
        S.append(S[-1] - len(new_infected))
        I.append(len(infecteds)-len(new_recovered)+len(new_infected))
        R.append(R[-1] + len(new_recovered))
        Allinfecteds.extend(list(new_recovered))
        
        for re in new_recovered:
            infecteds.remove(re)
        for ad in new_infected:
            infecteds.add(ad)
    
    return S,I,R,Allinfecteds    


def SIR_WG_TS_Nodes(G,alpha,mu,infecteds,tmin = 0, tmax = float('Inf')):
    """    
    numerical simulation of SIR model on a weighted network 
    
    it return nodes with different state at each time steps 
    
    Parameters
    ----------
    G : graph
        network.
    alpha : float
        infection rate.
    mu : float
        recover rate.
    infecteds : int
        initial seed .
    tmin : int, optional
        start time. The default is 0.
    tmax : TYPE, optional
        maximum infection times. The default is float('Inf').

    Returns
    -------
    S : list
        susceptible nodes over t.
    I : list
        infected nodes over t.
    R : list
        recovered node over t.
    """
    if G.has_node(infecteds):
        infecteds = set([infecteds])

    t = [0]
    S = [list(set(G.nodes())-set(infecteds))]
    I = [list(infecteds)]
    R = []
    
    node_state = defaultdict(lambda : ([tmin], ['S'])) #record the state of nodes 
    susceptible = defaultdict(lambda : True)  #record the susceptible nodes  
    
    for u in infecteds:
        node_state[u][0].append(t[-1])
        node_state[u][1].append('I')
        susceptible[u] = False
    
    while infecteds and t[-1] < tmax-1:
        
        #update the time
        t.append(t[-1]+1) 
        new_infected = set()
        new_recovered = set()
        
        for u in infecteds:
            for v in G.neighbors(u):
                if susceptible[v]:
                   weight = G.get_edge_data(u,v)['weight']
                   p = 1-np.power((1-alpha),weight)
                   if node_state[u][1][-1] == 'I' and _simple_transmission_(p):
                       node_state[v][1].append('I')
                       node_state[v][0].append(t[-1])
                       susceptible[v] = False
                       new_infected.add(v)
                       
            if  _simple_transmission_(mu):
                node_state[u][1].append('R') 
                node_state[u][0].append(t[-1])
                new_recovered.add(u)  
                
        S.append(list(set(S[-1])-set(new_infected)))
        I.append(list((set(infecteds)-set(new_recovered)) | set(new_infected)))
        R.append(list(set(new_recovered)))
        
        for re in new_recovered:
            infecteds.remove(re)
        for ad in new_infected:
            infecteds.add(ad)
        
    return S,I,R

def NumberOfinfected(allinfecteds,Wnodes):
    '''
    calculate  the total number of infected node in the original network

    Parameters
    ----------
    allinfecteds : list
        all infected nodes in a MC simulation.
    Wnodes : dict
        node weight, means the number of nodes contained by a new node.

    Returns
    -------
    Ninfecteds : int
        The total number of infected node in the original network.

    '''
    Ninfecteds = 0
    for infecteds in allinfecteds:
        Ninfecteds += Wnodes[infecteds]
    
    return Ninfecteds

def run_SIR_simulation(args):
    
    it_num,G,alphas,mu = args
    print('simulation:%d'%it_num)
    
    random_seed = random.choice(list(G.nodes))
    rhos = []
    
    for j,alpha in enumerate(alphas):
        [S,I,R,Allinfecteds] =SIR(G,alpha,mu,random_seed)
        rhos.append(R[-1])
    
    return rhos

def run_SIR_mseed(args):
    '''
    SIR numerical simulation over multiple seeds

    Parameters
    ----------
    args : list
        parameter sets.

    Returns
    -------
    rhos : list
        the number of final recovered nodes.
    '''
    it_num,G,alphas,mu,seeds = args
    print('simulation:%d'%it_num)
    rhos = []
    
    for j,alpha in enumerate(alphas):
        [S,I,R,Allinfecteds] =SIR(G,alpha,mu,seeds)
        rhos.append(R[-1])
    
    return rhos

def SIR_Sentinel(G, alpha, mu, sentinels,infecteds,tmin = 0, tmax = float('Inf')): 
    """
    
    Parameters
    ----------
    G : nx.graph()
        undirected connective network.
    alpha : float 
        infection rate, alpha in [0,1]
    mu : float 
        recover rate, mu in [0,1]
    infecteds : int
        initial infected seed node.
    tmin : int, optional
        initial time. The default is 0.
    tmax : TYPE, optional
        maximum excuation time. The default is float('Inf').

    Returns
    -------
    S : list
        the number of susceptible node from begin to end.
    I : list
        the number of infected node from begin to end.
    R : list
        the number of recovered node from begin to end.

    """
    sentinels_set = set(sentinels)
    infected_time = -1
    earlest_detection_model = False
    
    N = G.order()
    if type(infecteds) == list:
       infecteds = set(infecteds)
    elif G.has_node(infecteds):
       infecteds = set([infecteds])
    
    t = [0]
    S = [N-len(infecteds)]
    I = [len(infecteds)]
    R = [0]
    Allinfecteds = []
    
    node_state = defaultdict(lambda : ([tmin], ['S'])) #record the state of nodes 
    susceptible = defaultdict(lambda : True)  #record the susceptible nodes  
    
    for u in infecteds:
        node_state[u][0].append(t[-1])
        node_state[u][1].append('I')
        susceptible[u] = False
        if earlest_detection_model == False and (u in sentinels_set):
           infected_time = t[-1]
           earlest_detection_model = True
           
    while infecteds and t[-1] < tmax-1:#infecteds and 
        
        #update the time
        t.append(t[-1]+1) 
        new_infected = set()
        new_recovered = set()
        
        for u in infecteds:
            for v in G.neighbors(u):
                if susceptible[v]:
                   if node_state[u][1][-1] == 'I' and _simple_transmission_(alpha):
                       node_state[v][1].append('I')
                       node_state[v][0].append(t[-1])
                       susceptible[v] = False
                       new_infected.add(v)
                       
            if  _simple_transmission_(mu):
                node_state[u][1].append('R') 
                node_state[u][0].append(t[-1])
                new_recovered.add(u)
                
        S.append(S[-1] - len(new_infected))
        I.append(len(infecteds)-len(new_recovered)+len(new_infected))
        R.append(R[-1] + len(new_recovered))
        Allinfecteds.extend(list(new_recovered))
        
        for re in new_recovered:
            infecteds.remove(re)
        for ad in new_infected:
            infecteds.add(ad)
            if earlest_detection_model == False and (ad in sentinels_set):
                infected_time = t[-1]
                earlest_detection_model = True
          
    if earlest_detection_model == False and infected_time == -1:
       return t[-1]
    else:
       return infected_time

def run_SIR_Sentinel_mseed(args):
    
    it_num,G,alphas,mu,sentinels = args
    print('simulation:%d'%it_num)
    sentinel_time = []
    
    seeds = random.choice(list(G.nodes))
    for j,alpha in enumerate(alphas):
        first_time = SIR_Sentinel(G,alpha,mu,sentinels,seeds)
        sentinel_time.append(first_time)

    return sentinel_time
    
def run_SIRWG_simulation(args):
    '''
    The function could be run with paralell computing 
    
    Parameters
    ----------
    args : 
       include all parameters: it_num,G,Wnodes,alphas,mu.
       it_num: simulation times
       RG: k-clique CGNs
       OG: original networks
       Wnodes: node weight, denotes the number of nodes contained by a new reduced node
       alphas: infection rate, array 
       mu: recover rate
           
    Returns
    -------
    Orhos : list 
        the number of recovered node for different $\beta$ in the original network.
    Rrhos : list
        the number of recovered node for different $\beta$ in the unweight reduced network.
    Rwrhos: list
        the number of recovered node for different $\beta$ in the weight reduced network.

    '''
    
    it_num,RG,OG,Wnodes,alphas,mu = args
    print('simulation:%d'%it_num)
    
    random_seed = random.choice(list(set(RG.nodes) & set(OG.nodes)))
    Orhos = []
    Rrhos = []
    Rwrhos = []
    
    for j,alpha in enumerate(alphas):
        [OS,OI,OR,OAllinfecteds] = SIR(OG,alpha,mu,random_seed)
        [RS,RI,RR,RAllinfecteds] = SIR(RG,alpha,mu,random_seed)
        [RwS,RwI,RwR,RwAllinfecteds] =SIR_WG(RG,alpha,mu,random_seed)
        Orhos.append(OR[-1])
        Rrhos.append(NumberOfinfected(RAllinfecteds,Wnodes))
        Rwrhos.append(NumberOfinfected(RwAllinfecteds,Wnodes))
    
    return Orhos, Rrhos, Rwrhos

def parseResult_SIRWG(results,path,Sclique):
    '''
    Parse the result 

    Parameters
    ----------
    results : list
        result obtained from the original network, reduced network and weight reduced network.
    path : str
        path to save the result.
    Sclique : int
        value of (k-1)-clique CGNs.

    Returns
    -------
    Omc : array
        results of numerical simulation on original networks.
    Rmc : array
        results of numerical simulation on reduced networks.
    RWGmc : array
        results of numerical simulation on weight reduced networks.

    '''
    Onet_mc = []
    Rnet_mc = []
    RWGnet_mc = []
    for Ons, Rns, Rwgs in results:
         Onet_mc.append(Ons)
         Rnet_mc.append(Rns)
         RWGnet_mc.append(Rwgs)
    
    pk.dump(np.array(Onet_mc),open(path+'/'+str(Sclique)+'_Onetmc','wb'))
    pk.dump(np.array(Rnet_mc),open(path+'/'+str(Sclique)+'_Rnetmc','wb'))
    pk.dump(np.array(RWGnet_mc),open(path+'/'+str(Sclique)+'_RWGnetmc','wb'))

    Omc = np.average(np.array(Onet_mc),axis=0)
    Rmc = np.average(np.array(Rnet_mc),axis=0)
    RWGmc= np.average(np.array(RWGnet_mc),axis=0)
    
    return Omc, Rmc, RWGmc

def networkReducted(edgelist,Wnodes,Wedges,Rorder,file_name):
    """
    reduce the network into k-clique CGNs with k-clique
    
    Parameters
    ----------
    edgelist : list
        The graph is described by the edgelist, e.g. v1 v2 weight. It is a undirected graph.
    Wnodes : dict
        The number of old nodes contained by a new reducted node. 
        In the initial condition, every node contains a node, e.g. v1:1,v2:1,....
    Wedges: dict
        The number of edge between two nodes            
    Rorder : int
        The size of reduced clique. Once the Rorder is given, the clique whose size is Rorder will be merge into a new node. 
    file_name : str
        path and file name to save.

    Returns
    -------
    reducedEdgelist : array 
        a new edgelist generated by reducing network after the size of clique is given, e.g. [s1 t1 w1],[s2,t2,w2]
    Wnewnodes : dict
        the node weight: every node contains the number of old node.
    """
    
    G = load_network(edgelist)    
    newNode = max(G.nodes())+1
    clique_label = {} #store new nodes 
    Snodes = set() #set of reducted nodes
    
    #1.create new nodes
    for clique in nx.find_cliques(G):
        if len(clique) == Rorder:
            #satisfy the a given Rorder , decide whether the clique should be reduced
            i = 0
            for node in clique:
                if node not in Snodes:
                    i = i + 1
                else:
                    break
                
            if i == Rorder:
                for node in clique:
                    Snodes.add(node) 
                    
                clique_label[newNode] = clique
                newNode = newNode + 1
        
    #2. build the map between old nodes and new nodes 
    OldtoNew = {}
    Wnewnodes = defaultdict(lambda:0) #update node weight
    for newnode in clique_label.keys():
        for node in clique_label[newnode]:
            OldtoNew[node]=newnode
            Wnewnodes[newnode] += Wnodes[node]  #update the node weight for new nodes 
    
    for node in G.nodes():
        if node not in Snodes:
            Wnewnodes[node] = Wnodes[node]
            
    #3 update edgelist and build the map between new edge and old edge 
    NewEdgetoOld = {}
    OldEdgetoNew = {}
    newEdgelist = []
    
    for edge in edgelist:
        new_item = []
        save = 0   #determine whether to save the newe edge map
        for node in edge:
            if node not in Snodes: #the node is not in the k-clique, add the original name 
                new_item.append(node)
            else:
                new_item.append(OldtoNew[node])
                save = 1
                
        sort_nedge = sorted(new_item)
        nedge = (sort_nedge[0],sort_nedge[1]) #sorted the edge and save in tuple
        
        if save == 1:   
            
            sort_oedge = sorted(edge)
            oedge = (sort_oedge[0],sort_oedge[1])
            
            if NewEdgetoOld.get(nedge) == None:
                NewEdgetoOld[nedge]=set()
                NewEdgetoOld[nedge].add(oedge)
            else:
                NewEdgetoOld[nedge].add(oedge)
                
            if OldEdgetoNew.get(oedge) == None:
                OldEdgetoNew[oedge]=set()
                OldEdgetoNew[oedge].add(nedge)
            else:
                OldEdgetoNew[oedge].add(nedge)    
    
        newEdgelist.append(nedge)
    
    #4. update the edge weight and edgelist 
    reducedEdgelist = []
    Wnewedges = {}
    
    for newedge in newEdgelist:
        if newedge in NewEdgetoOld.keys():
            if Wnewedges.get(newedge) == None:
                Wnewedges[newedge] = 0
                for oldedge in NewEdgetoOld[newedge]:
                    Wnewedges[newedge] += Wedges[oldedge]
        else:
            Wnewedges[newedge] = Wedges[newedge]
            reducedEdgelist.append([newedge[0],newedge[1],Wedges[newedge]])
    
    for newedge in NewEdgetoOld.keys():
           if newedge[0] != newedge[1]:
               reducedEdgelist.append([newedge[0],newedge[1],Wnewedges[newedge]])
            
    pk.dump(reducedEdgelist,open(file_name+'_reducedEdgelist','wb'))
    
    pk.dump(dict(Wnewnodes),open(file_name+'_Wnewnodes','wb'))
    pk.dump(OldtoNew,open(file_name+'_OldtoNew','wb'))
    pk.dump(clique_label,open(file_name+'_clique_label','wb'))
    
    pk.dump(dict(Wnewedges),open(file_name+'_Wnewedges','wb'))
    pk.dump(dict(NewEdgetoOld),open(file_name+'_NewEdgetoOld','wb'))
    pk.dump(dict(OldEdgetoNew),open(file_name+'_OldEdgetoNew','wb'))

    return np.array(reducedEdgelist), dict(Wnewnodes),Wnewedges

def coarseGrain(edgelist,Rorder,save_path,maxStep = 300000):
    '''
    run the coarsing grain process for a given network 
    
    Parameters
    ----------
    edgelist : list
        network list, a row denotes an edge.
    Rorder : int
        the value of k-clique CGNs.
    save_path :  str
        path to load the network and save the result.
    maxStep : int, optional
        maximum reductional steps. The default is 300000.

    Returns
    -------
    Nnodes_array : array
        the number of nodes in CGNs over reduction steps t.
    Nedges_array : array
        the number of edges in CGNs over reduction steps t.
    sizeCliques_array : TYPE
        the size of maximum cliques in CGNs over reduction steps t.

    '''
    
    G = load_network(edgelist)
    Wnodes = {node:1 for node in G.nodes()} #node weight:record the number of nodes contained in a new node
    Wedges = {(sorted(edge)[0],sorted(edge)[1]):1 for edge in G.edges()}
    edgelists = np.array([[each[0], each[1], 1] for each in nx.to_edgelist(G)])
    pk.dump(edgelists,open(save_path + '/0_reducedEdgelist','wb'))
    
    #innitial value, original network
    t = 0    
    Csizeclique = max(len(clique) for clique in nx.find_cliques(G))
    sizeCliques = [Csizeclique]
    Nnodes = [G.order()] 
    Nedges = [G.size()]
            
    while t < maxStep and Csizeclique >= Rorder:
        
        t += 1 
        print('Coarse Grain:',t)
        
        file_name = save_path + '/' + str(t)
        [Edgelist, Wnewnodes, Wnewedges]= networkReducted(edgelists[:,0:2],Wnodes,Wedges,Csizeclique,file_name)
        
        edgelists = Edgelist.copy()
        Wnodes = Wnewnodes.copy()
        Wedges = Wnewedges.copy()
        
        G = load_network(edgelists[:,0:2]) #load the updated edgelist
        Csizeclique = max(len(clique) for clique in nx.find_cliques(G))
        
        sizeCliques.append(Csizeclique)
        Nnodes.append(G.order())
        Nedges.append(G.size())
    
    Nnodes_array = np.array(Nnodes) 
    Nedges_array = np.array(Nedges)
    sizeCliques_array = np.array(sizeCliques)
    
    np.savetxt(save_path + '/Nnodes.txt',Nnodes_array, fmt='%d')
    np.savetxt(save_path + '/Nedges.txt',Nedges_array, fmt='%d')
    np.savetxt(save_path + '/sizeCliques.txt',sizeCliques_array, fmt='%d')    
    
    return Nnodes_array, Nedges_array, sizeCliques_array

def networkReductedKplex(edgelist,Wnodes,Wedges,maxKplexes,m,file_name):
    """
    
    reduce the network into k-clique CGNs with Kplex

    Parameters
    ----------
    edgelist : list
        The graph is described by the edgelist, e.g. v1 v2 weight. It is a undirected graph.
    Wnodes : dict
        The number of old nodes contained by a new reducted node. 
        In the initial condition, every node contains a node, e.g. v1:1,v2:1,.... 
    maxKplexes : list
        reduced Kplexes. Each kplex in the Kplexes will be merged into a new node.
    m: int
        The size of reducted Kplexes 
    file_name : str
        path and file name to save.

    Returns
    -------
    reducedEdgelist : array 
        a new edgelist generated by reducing network after the size of clique is given, e.g. [s1 t1 w1],[s2,t2,w2]
    Wnewnodes : dict
        the node weigth: every node contains the number of old node.
    """
    
    G = load_network(edgelist)    
    newNode = max(G.nodes())+1
    kplexes_label = {} #store new nodes 
    Snodes = set() #set of reducted nodes
    
    #1.create new nodes
    for Kplex in maxKplexes:
        if len(Kplex) == m:
            #satisfy the a given Rorder , decide whether the clique should be reduced
            i = 0
            for node in Kplex:
                if node not in Snodes:
                    i = i + 1
                else:
                    break
                
            if i == m:
                for node in Kplex:
                    Snodes.add(node)                     
                kplexes_label[newNode] = Kplex
                newNode = newNode + 1
        
    #2. build the map between old nodes and new nodes 
    OldtoNew = {}
    Wnewnodes = defaultdict(lambda:0) #update node weight
    for newnode in kplexes_label.keys():
        for node in kplexes_label[newnode]:
            OldtoNew[node]=newnode
            Wnewnodes[newnode] += Wnodes[node]  #update the node weight for new nodes 
    
    for node in G.nodes():
        if node not in Snodes:
            Wnewnodes[node] = Wnodes[node]
                        
    #3 update edgelist and build the map between new edge and old edge 
    NewEdgetoOld = {}
    OldEdgetoNew = {}
    newEdgelist = []
    
    for edge in edgelist:
        new_item = []
        save = 0   #determine whether to save the newe edge map
        for node in edge:
            if node not in Snodes: #the node is not in the k-clique, add the original name 
                new_item.append(node)
            else:
                new_item.append(OldtoNew[node])
                save = 1
                
        sort_nedge = sorted(new_item)
        nedge = (sort_nedge[0],sort_nedge[1]) #sorted the edge and save in tuple
        
        if save == 1:   
            
            sort_oedge = sorted(edge)
            oedge = (sort_oedge[0],sort_oedge[1])
            
            if NewEdgetoOld.get(nedge) == None:
                NewEdgetoOld[nedge]=set()
                NewEdgetoOld[nedge].add(oedge)
            else:
                NewEdgetoOld[nedge].add(oedge)
                
            if OldEdgetoNew.get(oedge) == None:
                OldEdgetoNew[oedge]=set()
                OldEdgetoNew[oedge].add(nedge)
            else:
                OldEdgetoNew[oedge].add(nedge)    
    
        newEdgelist.append(nedge)
    
    #4. update the edge weight and edgelist 
    reducedEdgelist = []
    Wnewedges = {}
    
    for newedge in newEdgelist:
        if newedge in NewEdgetoOld.keys():
            if Wnewedges.get(newedge) == None:
                Wnewedges[newedge] = 0
                for oldedge in NewEdgetoOld[newedge]:
                    Wnewedges[newedge] += Wedges[oldedge]
        else:
            Wnewedges[newedge] = Wedges[newedge]
            reducedEdgelist.append([newedge[0],newedge[1],Wedges[newedge]])
    
    for newedge in NewEdgetoOld.keys():
           if newedge[0] != newedge[1]:
               reducedEdgelist.append([newedge[0],newedge[1],Wnewedges[newedge]])
            
    pk.dump(reducedEdgelist,open(file_name+'_reducedEdgelist','wb'))
    
    pk.dump(dict(Wnewnodes),open(file_name+'_Wnewnodes','wb'))
    pk.dump(OldtoNew,open(file_name+'_OldtoNew','wb'))
    pk.dump(kplexes_label,open(file_name+'_kplexes_label','wb'))

    pk.dump(dict(Wnewedges),open(file_name+'_Wnewedges','wb'))
    pk.dump(dict(NewEdgetoOld),open(file_name+'_NewEdgetoOld','wb'))
    pk.dump(dict(OldEdgetoNew),open(file_name+'_OldEdgetoNew','wb'))

    return np.array(reducedEdgelist), dict(Wnewnodes),Wnewedges


def coarseGrain_kplex(edgelist,k,m,save_path,mode=True,n=None,maxStep = 100000):
    '''
    run the coarsing grain process for a given network through k-plex

    Parameters
    ----------
    edgelist : list 
      edge list of network.
    k : int
      k values, k determines the subgraph structure 
    m : int
      size of reducting to the smallest Kplex during the coarse-grain.
    save_path : str
      the path to save the result.
    mode: bool
      paralell compututing to find the kplex if True, a ordinary method to find the k-plex if False
      The default is True
    n: int
      randomly select n cliques as the greedy seed
      The default is None, it means that all cliques is used to as greedy seed
    maxStep : int, optional
      run the procedure in ma. The default is 100000.

    Returns
    -------
    Nnodes_array : array
        the number of nodes over reduction steps.
    Nedges_array : array
        the number of edges over reduction steps.
    sizeCliques_array : array
        size of maximum clique over reduction steps.
    sizeKplex_array : array
        size of kplex over reduction steps.

    '''
    
    G = load_network(edgelist)
    Wnodes = {node:1 for node in G.nodes()} #node weight:record the number of nodes contained in a new node
    Wedges = {(sorted(edge)[0],sorted(edge)[1]):1 for edge in G.edges()}
    edgelists = np.array([[each[0], each[1], 1] for each in nx.to_edgelist(G)])
    pk.dump(edgelists,open(save_path + '/0_reducedEdgelist','wb'))

    Csizeclique = max(len(clique) for clique in nx.find_cliques(G)) 
    if mode == True: #m, the size of k-plex, assigned a initial value and enter the while
        maxKplex = kplex.max_plexes_greedy_process(G, k,save_path,n)
    else:
        maxKplex = kplex.max_plexes_greedy(G, k,save_path)
    maxSizeKplex = max(len(kplex) for kplex in maxKplex)

    t = 0
    sizeCliques = [Csizeclique]
    Nnodes = [G.order()] 
    Nedges = [G.size()]
    sizeKplex = [maxSizeKplex]
    
    while t < maxStep and maxSizeKplex >= m:
        
        print('Coarse Grain:',t)
        t +=1
        
        file_name = save_path + '/' + str(t)
        [Edgelist, Wnewnodes,Wnewedges]= networkReductedKplex(edgelists[:,0:2],Wnodes,Wedges,maxKplex,maxSizeKplex,file_name)

        edgelists = Edgelist.copy()
        Wnodes = Wnewnodes.copy()
        Wedges = Wnewedges.copy()
        
        #iteration: maximum Kplex
        G = load_network(edgelists[:,0:2])
        Csizeclique = max(len(clique) for clique in nx.find_cliques(G))
        if mode == True:
            maxKplex = kplex.max_plexes_greedy_process(G, k,save_path,n)
        else:
            maxKplex = kplex.max_plexes_greedy(G,k,save_path)
        maxSizeKplex = max(len(kplex) for kplex in maxKplex)
        
        print('maxSizeKplex:',maxSizeKplex)

        sizeCliques.append(Csizeclique)
        Nnodes.append(G.order())
        Nedges.append(G.size())
        sizeKplex.append(maxSizeKplex)

    Nnodes_array = np.array(Nnodes)
    Nedges_array = np.array(Nedges)
    sizeCliques_array = np.array(sizeCliques)
    sizeKplex_array = np.array(sizeKplex)
    
    np.savetxt(save_path + '/Nnodes.txt',Nnodes_array, fmt='%d')
    np.savetxt(save_path + '/Nedges.txt',Nedges_array, fmt='%d')
    np.savetxt(save_path + '/sizeCliques.txt',sizeCliques_array, fmt='%d')    
    np.savetxt(save_path + '/sizeKplex.txt',sizeKplex_array,fmt = '%d')
    
    return Nnodes_array,Nedges_array,sizeCliques_array,sizeKplex_array

def coarseGrain_approxiate(edgelist,p,m,save_path,n=None,maxStep = 100000):
    '''
    Approximately coarse-graining network 

    Parameters
    ----------
    edgelist : list
        edge list.
    p : float
        probability to tune the size of kplex.
    m : int
        minimum value of k-plex size 
    save_path : str
        str to save the result.
    n : TYPE, optional
        the finding space of k-plex, if the n is larger, it means that 
        there is large probability to find the larger kplex, but it computational time is high.
        The default is None.
    maxStep : TYPE, optional
        maximum step to perform coarse-grain process. The default is 100000.

    Returns
    -------
    Nnodes_array : array
        the number of nodes over reduction steps.
    Nedges_array : array
        the number of edges over reduction steps.
    sizeCliques_array : array
        size of cliques over reduction steps.
    sizeKplex_array : array
        size of kplex over reduction steps.
    K_array : array
        different k value for k-plex over reduction steps.

    '''
    G = load_network(edgelist)
    Wnodes = {node:1 for node in G.nodes()} #node weight:record the number of nodes contained in a new node
    Wedges = {(sorted(edge)[0],sorted(edge)[1]):1 for edge in G.edges()}
    edgelists = np.array([[each[0], each[1], 1] for each in nx.to_edgelist(G)])
    pk.dump(edgelists,open(save_path + '/0_reducedEdgelist','wb'))
    
    Csizeclique = max(len(clique) for clique in nx.find_cliques(G))
    k = math.floor(Csizeclique*p)
    maxKplex = kplex.max_plexes_greedy_process(G,k,save_path,n)
    maxSizeKplex = max(len(kplex) for kplex in maxKplex)

    t = 0
    sizeCliques = [Csizeclique]
    Nnodes = [G.order()] 
    Nedges = [len(G.edges())]
    sizeKplex = [maxSizeKplex]
    K = [k] 
    
    while t < maxStep and maxSizeKplex >= m:
        
        
        print('Coarse Grain:',t)
        t +=1
        
        file_name = save_path + '/' + str(t)
        [Edgelist, Wnewnodes,Wnewedges]= networkReductedKplex(edgelists[:,0:2],Wnodes,Wedges,maxKplex,maxSizeKplex,file_name)
        
        edgelists = Edgelist.copy()
        Wnodes = Wnewnodes.copy()
        
        #iteration: maximum Kplex
        G = load_network(np.array(edgelists)[:,0:2])
        Csizeclique = max(len(clique) for clique in nx.find_cliques(G))
        k = math.floor(Csizeclique*p)
        if k >= 3:
            maxKplex = kplex.max_plexes_greedy_process(G, k,save_path,n)
            maxSizeKplex = max(len(kplex) for kplex in maxKplex)
        else:
            maxKplex = list(nx.find_cliques(G))
            maxSizeKplex = Csizeclique
            
        sizeCliques.append(Csizeclique)
        Nnodes.append(G.order())
        Nedges.append(len(G.edges()))
        sizeKplex.append(maxSizeKplex)
        K.append(k)
        
        print('reduction steps:%d. maxSizeKplex: %d. k:%d'%(t,maxSizeKplex,k))

    Nnodes_array = np.array(Nnodes)
    Nedges_array = np.array(Nedges)
    sizeCliques_array = np.array(sizeCliques)
    sizeKplex_array = np.array(sizeKplex)
    K_array = np.array(K)
    
    np.savetxt(save_path + '/Nnodes.txt',Nnodes_array, fmt='%d')
    np.savetxt(save_path + '/Nedges.txt',Nedges_array, fmt='%d')
    np.savetxt(save_path + '/sizeCliques.txt',sizeCliques_array, fmt='%d')    
    np.savetxt(save_path + '/sizeKplex.txt',sizeKplex_array,fmt = '%d')
    np.savetxt(save_path + '/K.txt',K_array,fmt = '%d')

    return Nnodes_array,Nedges_array,sizeCliques_array,sizeKplex_array,K_array
    
def calculate_reduce_ratio(current_max_clique_order,current_number_node):
    
    total = current_number_node[0]
    ratio = current_number_node/total
    order_set = sorted(list(set(current_max_clique_order)))
    order = {}
    for i in order_set:
        step = np.where(current_max_clique_order==i)[0]
        order[i] = ratio[np.min(step)]               
    
    return order

def calculate_reduce_step(current_max_clique_order):
    
    order_set = sorted(list(set(current_max_clique_order)))
    seq_index = []
    for i in order_set:
        step = np.where(current_max_clique_order==i)[0]
        seq_index.append(np.min(step))
    
    seq_step = {}
    for i in np.arange(len(order_set)-1):
        seq_step[order_set[i+1]] = seq_index[i]-seq_index[i+1]
    
    return seq_step

def PlotAxes(ax,xlabel,ylabel,title=''):
    
    font_label = {'family': "Arial", 'size':24}
    n_legend = 20
    ax.set_xlabel(xlabel,  fontdict = font_label)
    ax.set_ylabel(ylabel, fontdict = font_label)
    ax.set_title(title, loc='left',fontdict = {'family': "Arial", 'size':30,'weight':'demibold'})
    ax.tick_params(direction='in', which='both',length =3, width=1, labelsize=n_legend)
    
def swaprow(data):
    
    transform = data.T.copy()
    Nrow = transform.shape[0]
    for i in np.arange(Nrow/2):
       temp = transform[int(i),:].copy()
       transform[int(i),:] = transform[int(Nrow-1-i),:].copy()
       transform[int(Nrow-1-i),:] =temp
       
    return transform

def OccupyCliqueMu():
    
    beta = sympy.symbols('beta')
    mu = sympy.symbols('mu')
    
    s = 1-beta
    y2 =  beta/(beta+mu-beta*mu)
    y3 =  (np.power(beta,2) + 2*np.power(beta,2)*(2-mu)*s/(1-(1-mu)*s))
    
    return [y2,y3]

def calculate_flucation(x):

     s = np.average(x)
     if np.average(np.power(x,2))-np.power(s,2)<0:
         a = 0
     else:    
         a = np.sqrt(np.average(np.power(x,2))-np.power(s,2)) 
     return a / s

def cos_sim(a, b):
    
    a_norm = np.linalg.norm(a)
    b_norm = np.linalg.norm(b)
    cos = np.dot(a,b)/(a_norm * b_norm)
    return cos

def calculate_frequence(number_of_infected):
    
    L = list(number_of_infected)
    freq = {str(i):L.count(i)/len(L) for i in L} 
    
    return freq

def num_of_inf_distribution(num_node,number_of_infected):
    
    ioi_freq = calculate_frequence(number_of_infected)
    
    accumulate = np.zeros(int(num_node)+1)
    for keys, p in ioi_freq.items():
        accumulate[int(float(keys))] = p
 
    return accumulate

def InfectProb(G, alpha, mu, infecteds, simulations):
    '''
    calculate the probability to be infected for every nodes in the network 

    Parameters
    ----------
    G : graph
        original network.
    alpha : float
        infection rate.
    mu : float
        recover rate.
    infecteds : int or list
        infected nodes.
    simulations : int
        simulation times.

    Returns
    -------
    Ninfected : dict
        the probability to be infected for every nodes in the network 
        except for the seed node.

    '''
    Ninfected = {node: 0 for node in G.nodes()}
    for itr in np.arange(simulations):
        [S,I,R,Allinfecteds] = SIR(G,alpha,mu,infecteds)
        for infected in Allinfecteds:
            Ninfected[infected]+=1
    
    for node in Ninfected.keys():
        Ninfected[node] = Ninfected[node]/simulations
    
    return Ninfected
 
def TrackBackNodes(Allinfecteds,cliquelabels):
    '''
    track the nodes contained by supernode 

    Parameters
    ----------
    Allinfecteds : list
        infected nodes in the reducted network.
    cliquelabels : dict
        map relation of names between supernode and node.

    Returns
    -------
    Allnodes : list
        all infected nodes in the original network.

    '''
    ReductedNodes = set(list(cliquelabels))
    Allnodes = []
    Stack=[]
    for node in Allinfecteds:
        if node in ReductedNodes:
            Stack.append(node)
        else:
            Allnodes.append(node)
    
    while len(Stack) != 0:
        reducted = Stack.pop()
        for node in cliquelabels[reducted]:
            if node in ReductedNodes:
                Stack.append(node)
            else: 
                 Allnodes.append(node)
    
    return Allnodes
 
def TrackBackEdges(AllRedges,AllEdgesMap):
    '''
    track the edges contained by superedge 

    Parameters
    ----------
    AllRedges : list
        superedges in the reducted network..
    AllEdgesMap : dict
        map relation of names between superedge and edge.

    Returns
    -------
    Alledges : list
        all edges in the original network..

    '''
    ReducedEdges = set(AllEdgesMap.keys())
    Alledges = []
    Stack = []
    for edge in AllRedges:
        if edge in ReducedEdges:
           Stack.append(edge)
        else:
           Alledges.append(edge)
           
    while len(Stack) != 0:
      reducted = Stack.pop()
      for edge in AllEdgesMap[reducted]:
          if edge in ReducedEdges:
              Stack.append(edge)
          else: 
               Alledges.append(edge)
    
    return Alledges

def InfectProbWG(OG, RG, alpha, mu, infecteds, simutimes, cliquelabels, weight):
    '''
    calculate the probability of every nodes being infected in the k-clique CGNs.

    Parameters
    ----------
    OG : graph
        original network.
    RG : graph
        reducted network.
    alpha : float
        infection rate.
    mu : float
        recover rate.
    infecteds : int or list
        infected nodes.
    simulations : int
        simulation times.
    cliquelabels : dict
        label of all new nodes .
    weight : bool
        network type.
        
    Returns
    -------
    Ninfected : dict
        the probability to be infected for every nodes in the network 
        except for the seed node.

    '''

    Ninfected = {node: 0 for node in OG.nodes()}
    if weight == True:
        for itr in np.arange(simutimes):
            [S,I,R,Allinfecteds] = SIR_WG(RG,alpha,mu,infecteds)
            Allnodes = TrackBackNodes(Allinfecteds, cliquelabels)
            for infected in Allnodes:
                Ninfected[infected]+=1
                
        for node in Ninfected.keys():
            Ninfected[node] = Ninfected[node]/simutimes             
        
        return Ninfected

    else:
        for itr in np.arange(simutimes):
            [S,I,R,Allinfecteds] = SIR(RG,alpha,mu,infecteds)
            Allnodes = TrackBackNodes(Allinfecteds, cliquelabels)
            for infected in Allnodes:
                Ninfected[infected]+=1
        
        for node in Ninfected.keys():
            Ninfected[node] = Ninfected[node]/simutimes
        
        return Ninfected

def NumNewClique(path, netIndex):
    '''
    count the number of new clique formed by reduction process

    Parameters
    ----------
    path : str
        path to load the cliquelabel.
    netIndex : dict
        relation between the network and index.

    Returns
    -------
    NewCliqueNum : dict
        the number of new formed clique for different size of clique.
    NewClique_sample : list
        sample of clique.

    '''
    NewCliqueNum = {}
    NewClique_sample = []
    cliques = sorted(netIndex.keys(),reverse =True)
    
    i = 0
    while i < len(cliques)-1:
        
        current_clique = cliques[i]
        next_clique = cliques[i+1]
    
        current_inx  = netIndex[current_clique] 
        next_inx = netIndex[next_clique] 
       
        if current_inx+1 == next_inx:
            NewCliqueNum[current_clique] = 0
        else:
            count = 0
            for each_inx in np.arange(current_inx+1,next_inx):
                cliquelabel = load(path+'/'+str(each_inx)+'_clique_label')
                count = count + len(cliquelabel)
                NewClique_sample.extend([current_clique]*len(cliquelabel))
            NewCliqueNum[current_clique] = count
        
        i = i+1
    
    return  NewCliqueNum,NewClique_sample

def ReducedNodeMap(path):
    """
    map raltion between supernodes and nodes in the original networks

    Parameters
    ----------
    path : str
        The path to load the sizeCliques and cliquelabels.

    Returns
    -------
    cliquelabels : dict
        All cliquelabels that record all map relations, e.g.Newnode -> Oldnode

    """
    #load all map relations between new and old nodes
    sizeCliques = np.loadtxt(path + '/sizeCliques.txt')
    cliquelabels = {}
    for inx in np.arange(1,len(sizeCliques)):
        cliquelabel = load(path+'/'+str(inx)+'_clique_label')
        cliquelabels.update(cliquelabel)
    
    return cliquelabels

def ReducedEdgeMap(path):
    """
    map raltion between superedges and edges in the original networks

    Parameters
    ----------
    path : str
        The path to load the sizeCliques and cliquelabels.

    Returns
    -------
    AlledgesMap : dict
        All cliquelabels that record all map relations, e.g.newedge -> oldedge

    """
    sizeCliques = np.loadtxt(path + '/sizeCliques.txt')
    AlledgesMap = {}
    for inx in np.arange(1,len(sizeCliques)):
        edgemap = load(path+'/'+str(inx)+'_NewEdgetoOld')
        AlledgesMap.update(edgemap)
    
    return  AlledgesMap
     
def kendallTau(p,q):
    '''
    calculate the kendallTau coefficient

    Parameters
    ----------
    p : Series
        probability distribution.
    q : Series
        probability distribution.

    Returns
    -------
    float
        KL value.

    '''
    return scipy.stats.pearsonr(p, q)

def VectorNorm(p,q):
    '''
    calculate the the Euclidean distance of vector

    Parameters
    ----------
    p : array
        a vector formed by being infected probability of each node.
    q : array
        a vector formed by being infected probability of each node.

    Returns
    -------
    TYPE
        value of distance.

    '''
    m = np.array(list(p.values()))
    n = np.array(list(q.values()))
    x= (1/np.linalg.norm(m)*m) - (1/np.linalg.norm(n)*n)
    return  np.linalg.norm(x)

def OneInfectedSeed(args):
    '''
    For single nodes
    calculate the the Euclidean distance of vector formed by being infected probability between original network and k-clique CGNs   
    
    Parameters
    ----------
    args : list
        parameter sets.

    Returns
    -------
    RGdist : float
        value of the Euclidean distance of vector on reduced networks.
    RGwdist : float
        value of the Euclidean distance of vector on weighted reduced networks.

    '''
    OG,RG,alpha,mu,infected,simutimes,cliquelabels =args
    OGProb = InfectProb(OG, alpha, mu, infected, simutimes)
    RGProb = InfectProbWG(OG,RG,alpha,mu,infected,simutimes,cliquelabels,weight=False)
    RGwProb = InfectProbWG(OG,RG,alpha,mu,infected,simutimes,cliquelabels,weight=True)
    RGdist = VectorNorm(RGProb,OGProb)
    RGwdist= VectorNorm(RGwProb,OGProb)
    
    return RGdist, RGwdist

def VectorDist(OG,RG,alpha,mu,simutimes,Nseed,path):
    '''
    For different seeds,
    calculate the the Euclidean distance of vector formed by probability of each node to be infected in the original network and k-clique CGNs 
    
    Parameters
    ----------
    OG : nx.graph
        original network.
    RG : nx.graph
        reducted network.
    alpha : float
        infection rate.
    mu : float
        recover rate.
    simutimes : int
        simulation times.
    Nseed : int
        the numer of seed nodes.
    path : str
        the path to load the cliquelabels.

    Returns
    -------
    vss : list
        distance of vector formed by infected probability of nodes between original and reducted network.
    wvss : list
        distance of vector formed by infected probability of nodes between original and weight reducted network.

    '''
    cliquelabels = ReducedNodeMap(path)
    seeds = set(RG.nodes()) & set(OG.nodes())
    
    vss = []
    wvss = []
    
    args = []
    for itr in np.arange(Nseed):
        infected = random.choice(list(seeds))
        args.append([OG,RG,alpha,mu,infected,simutimes,cliquelabels])
    
    pool = Pool(processes = 10)
    results = pool.map(OneInfectedSeed,args) 
    
    for rgdist,rgwdist in results:
        vss.append(rgdist)
        wvss.append(rgwdist)
        
    return vss,wvss

def ProbOneReducedNet(args):
    '''
    For different alpha, calculate the probability of nodes being infected in the original network and k-clique CGNs
    and further quantify the difference between original and reduced networks using the Euclidean distance 
    
    This function could be run with paralell computing
    
    Parameters
    ----------
    its :int
        value of k-clique
    OG : graph
        original network.
    RG : graph
        reducted network.
    alphas : array
        infection rates.
    simutimes: int
        the simulation times    
    path : str
        the path to load the cliquelabels.

    Returns
    -------
    vssInfAvg : array
        average of vector distance for different seeds with a given RG vs alpha
    vssInfStd : array
        stand error of vector distance for various seeds in a given RG vs alpha
    wvssInfAvg : array
        average of vector distance for different seeds with a given weight RG vs alpha
    wvssInfStd : array
        average of vector distance for different seeds with a given weight RG vs alpha

    '''
    its, OG, RG, alphas, simutimes, path = args
    mu = 1
    Nseed = 50
    
    vssInfAvg = np.zeros(len(alphas))
    vssInfStd = np.zeros(len(alphas))
    wvssInfAvg = np.zeros(len(alphas))
    wvssInfStd = np.zeros(len(alphas))
    
    for i,alpha in enumerate(alphas):
        [vs,wvss] = VectorDist(OG,RG,alpha,mu,simutimes,Nseed,path)
        vssInfAvg[i] = np.average(np.array(vs)) #average over different seed nodes
        vssInfStd[i] = np.std(np.array(vs))  #the std as the average 
        wvssInfAvg[i] = np.average(np.array(wvss))
        wvssInfStd[i] = np.std(np.array(wvss))
        
    return vssInfAvg,vssInfStd,wvssInfAvg,wvssInfStd  

def OneInfectedSeed_ProbValues(args):
    '''
    For one seeds,
    calculate the probability of each node being infected in the original network and k-clique CGNs.  
    
    Parameters
    ----------
    OG : nx.graph
        original network.
    RG : nx.graph
        reducted network.
    alpha : float
        infection rate.
    mu : float
        recover rate.
    infected: int
        initial seeds
    simutimes : int
        simulation times.
    cliquelabels: dict
        map relation between supernodes and nodes 

    Returns
    -------
    OGProb : dict
        the probability of node being infected in the original network
    RGProb : dict
        the probability of node being infected in the k-clique CGNs
    RGwProb: dict
        the probability of node being infected in the k-clique CGNs by considering the weight information of edges 
    '''
    OG,RG,alpha,mu,infected,simutimes,cliquelabels =args
    OGProb = InfectProb(OG, alpha, mu, infected, simutimes)
    RGProb = InfectProbWG(OG,RG,alpha,mu,infected,simutimes,cliquelabels,weight=False)
    RGwProb = InfectProbWG(OG,RG,alpha,mu,infected,simutimes,cliquelabels,weight=True)
    
    return OGProb, RGProb, RGwProb

def ManyInfectedSeed_ProbValues(OG,RG,alpha,mu,simutimes,Nseed,path):
    '''
    For multiply seeds,
    calculate the probability of each node being infected in the original network and k-clique CGNs.  
    
    Parameters
    ----------
    OG : nx.graph
        original network.
    RG : nx.graph
        reducted network.
    alpha : float
        infection rate.
    mu : float
        recover rate.
    simutimes : int
        simulation times.
    Nseed : int
        the numer of seed nodes.
    path : str
        the path to load the cliquelabels.

    Returns
    -------
    OGProb : list
        the probability of node being infected in the original network
    RGProb : list
        the probability of node being infected in the k-clique CGNs
    RGwProb: list
        the probability of node being infected in the k-clique CGNs by considering the weight information of edges 
    '''
    
    cliquelabels = ReducedNodeMap(path)
    seeds = set(RG.nodes()) & set(OG.nodes())
    
    OGProb = []
    RGProb = []
    RGwProb = []
    
    args = []
    for itr in np.arange(Nseed):
        infected = random.choice(list(seeds))
        args.append([OG,RG,alpha,mu,infected,simutimes,cliquelabels])
    
    pool = Pool(processes = 30)
    results = pool.map(OneInfectedSeed_ProbValues,args) 
    
    for oprob,rprob,rwprob in results:
        OGProb.append(oprob)
        RGProb.append(rprob)
        RGwProb.append(rwprob)
        
    return OGProb,RGProb,RGwProb

def ProbValue_OneReducedNet(args):
    '''
    For different alpha and multiply innitial seeds, 
    calculate the probability of each node being infected in the original, k-clique CGNs, and weighted k-clique CGNs
    This function could be run with paralell computing

    Parameters
    ----------
    its: int
      index of current numerical simulation 
    OG : graph
        original network.
    RG : graph
        reducted network.
    alphas : array
        infection rates.
    simutimes: int
        the simulation times    
    path : str
        the path to load the cliquelabels.

    Returns
    -------
    OGProb_alpha : dict
        the probability of node being infected in the original network
    RGProb_alpha : dict
        the probability of node being infected in the k-clique CGNs
    RGwProb_alpha: dict
        the probability of node being infected in the k-clique CGNs by considering the weight information of edges 
    '''
    
    its, OG, RG, alphas, simutimes, path = args
    mu = 1
    Nseed = 50
    
    OGProb_alpha = {}
    RGProb_alpha = {}
    RGwProb_alpha = {}
    
    for i,alpha in enumerate(alphas):
        print('alpha',alpha)
        [OGProb,RGProb,RGwProb] = ManyInfectedSeed_ProbValues(OG,RG,alpha,mu,simutimes,Nseed,path)
        OGProb_alpha[alpha] = OGProb
        RGProb_alpha[alpha] = RGProb
        RGwProb_alpha[alpha] = RGwProb
        
    return OGProb_alpha,RGProb_alpha,RGwProb_alpha  
    
def InfectProbT(G,simutimes):
    
    InfectProbt = {}
    for time in G.keys():
        if len(G[time])!=0:
           InfectProbt[time] = sum(G[time])/simutimes
        else:
           InfectProbt[time] = 0
    
    return InfectProbt

def SpreadOverT(args):
    
    Sclique,G,RG,alpha,mu,infected,simutimes,Wnodes = args
    
    Govert= defaultdict(lambda:[])
    RGovert= defaultdict(lambda:[])
    RWGovert = defaultdict(lambda:[])
    N = G.order()
    
    for itr in np.arange(simutimes):
       [S,I,R,Allinfecteds] = SIR(G, alpha, mu, infected)
       [Snodes,Inodes,Rnodes] = SIR_TS_Nodes(RG,alpha,mu,infected)
       [Swnodes,Iwnodes,Rwnodes] = SIR_WG_TS_Nodes(RG,alpha,mu,infected)
       
       for t,rnum in enumerate(R):
          Govert[t].append(rnum/N)
       for t,rnodes in enumerate(Rnodes):
          RGovert[t].append(NumberOfinfected(rnodes,Wnodes)/N)
       for t,rnodes in enumerate(Rwnodes):
          RWGovert[t].append(NumberOfinfected(rnodes,Wnodes)/N) 

    Gpt = InfectProbT(Govert,simutimes)
    RGpt = InfectProbT(RGovert,simutimes)      
    RWGpt = InfectProbT(RWGovert,simutimes)
    
    return Sclique, infected, Gpt, RGpt, RWGpt

def NodeWeightDegree(RG):
    '''
    Parameters
    ----------
    RG : graph
    weight network.

    Returns
    -------
    degree: dict.
    node degree
    '''    
    degree ={}
    for node in RG.nodes():
        k=0
        for neighbor in nx.neighbors(RG,node):
           k += RG.get_edge_data(node,neighbor)['weight']
        degree[node]=k
    
    return degree

def half_incidence(graph, ordering='blocks', return_ordering=False):
    """Return the 'half incidence' matrices of the graph.

    The resulting matrices have shape of (n, 2m), where n is the number of
    nodes and m is the number of edges.


    Params
    ------

    graph (nx.Graph): The graph.

    ordering (str): If 'blocks' (default), the two columns corresponding to
    the i'th edge are placed at i and i+m. That is, choose an arbitarry
    direction for each edge in the graph. The first m columns correspond to
    this orientation, while the latter m columns correspond to the reversed
    orientation. Columns in both blocks are sorted following graph.edges.
    If 'consecutive', the first two columns correspond to the two
    orientations of the first edge, the third and fourth row are the two
    orientations of the second edge, and so on. In general, the two columns
    for the i'th edge are placed at 2i and 2i+1. If 'custom', parameter
    custom must be a dictionary of pairs of the form (idx, (i, j)) where
    the key idx maps onto a 2-tuple of indices where the edge must fall.

    custom (dict): Used only when ordering is 'custom'.

    return_ordering (bool): If True, return a function that maps an edge id
    to the column placement. That is, if ordering=='blocks', return the
    function lambda x: (x, m+x), if ordering=='consecutive', return the
    function lambda x: (2*x, 2*x + 1). If False, return None.


    Returns
    -------

    (source, target), or (source, target, ord_function) if return_ordering
    is True.


    Notes
    -----

    Assumes the nodes are labeled by consecutive integers starting at 0.

    """
    numnodes = graph.order()
    numedges = graph.size()

    if ordering == 'blocks':
        src_pairs = lambda i, u, v: [(u, i), (v, numedges + i)]
        tgt_pairs = lambda i, u, v: [(v, i), (u, numedges + i)]
    if ordering == 'consecutive':
        src_pairs = lambda i, u, v: [(u, 2*i), (v, 2*i + 1)]
        tgt_pairs = lambda i, u, v: [(v, 2*i), (u, 2*i + 1)]
    if isinstance(ordering, dict):
        src_pairs = lambda i, u, v: [(u, ordering[i][0]), (v, ordering[i][1])]
        tgt_pairs = lambda i, u, v: [(v, ordering[i][0]), (u, ordering[i][1])]

    def make_coo(make_pairs):
        """Make a sparse 0-1 matrix.

        The returned matrix has a positive entry at each coordinate pair
        returned by make_pairs, for all (idx, node1, node2) edge triples.

        """
        coords = [pair
                  for idx, (node1, node2) in enumerate(graph.edges())
                  for pair in make_pairs(idx, node1, node2)]
        data = np.ones(len(coords))
        return sparse.coo_matrix((data, list(zip(*coords))),
                                 shape=(numnodes, 2*numedges))

    source = make_coo(src_pairs).asformat('csr')
    target = make_coo(tgt_pairs).asformat('csr')

    if return_ordering:
        if ordering == 'blocks':
            ord_func = lambda x: (x, numedges+x)
        elif ordering == 'consecutive':
            ord_func = lambda x: (2*x, 2*x+1)
        elif isinstance(ordering, dict):
            ord_func = lambda x: ordering[x]
        return source, target, ord_func
    else:
        return source, target   
    
def nb_matrix(graph, aux=False, ordering='blocks', return_ordering=False):
    """Return NB-matrix of a graph.

    If aux=False, return the true non-backtracking matrix, defined as the
    unnormalized transition matrix of a random walker that does not
    backtrack. If the graph has m edges, the NB-matrix is 2m x 2m. The rows
    and columns are ordered according to ordering (see half_incidence).

    If aux=True, return the auxiliary NB-matrix of a graph is the block
    matrix defined as

    B' = [0  D-I]
         [-I  A ]

    Where D is the degree-diagonal matrix, I is the identity matrix and A
    is the adjacency matrix. If the graph has n nodes, the auxiliary
    NB-matrix is 2n x 2n. The rows and columns are sorted in the order of
    the nodes in the graph object.

    Params
    ------

    graph (nx.Graph): the graph.

    aux (bool): whether to return the auxiliary or the true NB-matrix.

    ordering ('blocks' or 'consecutive'): ordering of the rows and columns
    if aux=False (see half_incidence). If aux=True, the rows and columns of
    the result will always be in accordance to the order of the nodes in
    the graph, regardless of the value of ordering.

    return_ordering (bool): if True, return the edge ordering used (see
    half_incidence).

    Returns
    -------

    matrix (scipy.sparse): (auxiliary) NB-matrix in sparse CSR format.

    matrix, ordering_func: if return_ordering=True.

    """
    if aux:
        degrees = graph.degree()
        degrees = sparse.diags([degrees[n] for n in graph.nodes()])
        ident = sparse.eye(graph.order())
        adj = nx.adjacency_matrix(graph)
        pseudo = sparse.bmat([[None, degrees - ident], [-ident, adj]])
        return pseudo.asformat('csr')

    else:
        # Compute the NB-matrix in a single pass on the non-zero elements
        # of the intermediate matrix.
        sources, targets, ord_func = half_incidence(
            graph, ordering, return_ordering=True)
        inter = np.dot(sources.T, targets).asformat('coo')
        inter_coords = set(zip(inter.row, inter.col))

        # h_coords contains the (row, col) coordinates of non-zero elements
        # in the NB-matrix
        h_coords = [(r, c) for r, c in inter_coords if (c, r) not in inter_coords]
        data = np.ones(len(h_coords))
        nbm = sparse.coo_matrix((data, list(zip(*h_coords))),
                                shape=(2*graph.size(), 2*graph.size()))

        # Return the correct format
        nbm = nbm.asformat('csr')
        return (nbm, ord_func) if return_ordering else nbm

def compute_mu(graph, val, vec):
    """Compute mu given the leading eigenpair.

    Params
    ------

    graph (nx.Graph): the graph.

    val (float): the leading eigenvalue.

    vec (np.array): the first half of the principal left unit eigenvector.

    Returns
    -------

    mu (float): a constant such that mu * vec is the 'correctly' normalized
    non-backtracking centrality.

    """
    degs = graph.degree()
    coef = sum(vec[n]**2 * degs(n) for n in graph)
    return np.sqrt(val * (val**2 - 1) / (1 - coef))

def nb_centrality(graph, normalized=True, return_eigenvalue=False, tol=0):
    
    """Return the non-backtracking centrality of each node.

    The nodes must be labeled by consecutive integers starting at zero.

    Params
    ------

    graph (nx.Graph): the graph.

    normalized (bool): whether to return the normalized version,
    corresponding to v^T P v = 1.

    return_eigenvalue (bool): whether to return the largest
    non-backtracking eigenvalue as part of the result.

    tol (float): the tolerance for eignevecto computation. tol=0 (default)
    means machine precision.

    Returns
    -------

    centralities (dict): dictionary of {node: centrality} items.

    centralities (dict), eigenvalue (float): if return_eigenvalue is True.

    """
    # Matrix computations require node labels to be consecutive integers,
    # so we need to (i) convert them if they are not, and (ii) preserve the
    # original labels as an attribute.
    graph = nx.convert_node_labels_to_integers(
        graph, label_attribute='original_label')

    # The centrality is given by the first entries of the principal left
    # eigenvector of the auxiliary NB-matrix
    val, vec = sparse.linalg.eigs(nb_matrix(graph, aux=True).T, k=1, tol=tol)
    val = val[0].real
    vec = vec[:graph.order()].ravel()

    # Sometimes the vector is returned with all negative components and we
    # need to flip the sign.  To check for the sign, we check the sign of
    # the sum, since every element has the same sign (or is zero).
    if vec.sum() < 0:
        vec *= -1

    # Currently, vec is unit length. The 'correct' normalization requires
    # that we scale it by \mu.
    if normalized:
        vec *= compute_mu(graph, val, vec)

    # Pack everything in a dict and return
    result = {graph.nodes[n]['original_label']: vec[n].real for n in graph}
    return (result, val) if return_eigenvalue else result

def collective_influence(graph):
    """Return the collective influence of each node.

    Here we only use the immediate neighborhood of each node to compute its
    collective influence, i.e. we use l = 1 in equation (1) of [1].

    Params
    ------

    graph (nx.Graph): the graph.

    Returns
    -------

    centralities (dict): dictionary of {node: centrality} items.

    References
    ----------

    [1] Morone, Flaviano, et al. "Collective influence algorithm to find
    influencers via optimal percolation in massively large social media."
    Scientific reports 6 (2016): 30062.

    """
    deg = graph.degree()
    return {node: (deg[node] - 1) * sum(deg[n] - 1 for n in graph.neighbors(node))
            for node in graph}
